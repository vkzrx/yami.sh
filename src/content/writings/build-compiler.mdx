---
layout: ../../layouts/writing.astro
title: "Crafting a Tokenizer"
description: "Learning about compilers by building a tokenizer in Rust."
author: Yami
date: 2023-07-18
draft: true
tags:
  - Rust
---

import FlowChart from "~/components/flow-chart.astro";
import TokenHighlighter from "~/components/token-highlighter.astro";

As a software engineer, I spend hours almost every day writing code. Whether I'm building a CLI, an HTTP server, a user interface... There was an aspect of writing software that was not very clear to me:

**How do we go from code to instructions that machines understand?**

In this post, we'll learn about the basics of compilers by building the very first component of a compiler in Rust, the tokenizer. More specifically, we'll build a tokenizer that understands the following code:

```rust
fn add(x: i32, y: i32) -> i32 {
    x + y
}

fn main() {
    let x = add(33, 9);
    println(x);
}
```

If you know Rust, you may have noticed that it looks pretty much the same. I've been learning Rust recently and its syntax has been very enjoyable to work with.

## A Primer on compilers

Writing code is typically done using a high-level programming language, such as Rust, Go or Javascript, which are just ways to communicate with machines. Compilers are used to translate our code into a language that machines can understand.

Here's an high-level overview of the compilation process:

<FlowChart items={["Tokenize", "Parse", "Analyze", "Generate"]} />

Note that this is a simplified version. A real-world compiler involves additional steps, such as intermediate representation (IR), optimization, error detection, and more.

## Lexical analysis

The lexical analysis is typically the first step of a compiler. Similar to natural languages, sequences of characters in code have meaning. A sequence is called a `lexeme`. For example, `println`, `(`, `if`, `=` and `fn` are all lexemes, and each of them is categorized into `tokens`. For instance, `fn`and `let` are classified as `Keyword` tokens, while `+`, `-` and `*` are considered `BinaryOperator` tokens. When combined, these tokens form a `grammar`.

Let's consider the following code:

```rust
fn main() {
    let x = 42;
}
```

The lexical analysis is performed by a component called the `Lexer`, which handles the process of converting a stream of characters (the source code) into a stream of tokens.

For the above code, the lexer would produce the following tokens:

<TokenHighlighter
  input={["fn", "main", "(", ")", "{", "\n", "let", "x", "=", "42", ";", "\n", "}"]}
/>

At this stage, we don't verify yet if those tokens are grammatically correct when combined.

## Parser

While the role of the lexer is to give meaning to sequences of characters through tokens, the role of the parser is to construct sentences using those tokens. Sentences are called `Statement` or `Expression`. As an example, `let x = 42;` is a variable declaration and falls under the `Statement` category. On the other hand, `3 + 39` is an `Expression`.

The output of the parser is called an `Abstract Syntax Tree` (AST). It's a way to represent the code in a structure that's easier to work with.
